# Getting Started
Here you will find the instructions to test the VVA Autonomous Mobile Robot, either in simulation or with a hardware vehicle.

## Pre-requisites
* ROS Melodic
* The package depth_nav_tools/laserscan_kinect is used for cliff detection, it is not available in ROS Melodic but it can be downloaded and compiled. [Get depth_nav_tools
](https://github.com/mdrwiega/depth_nav_tools)
* If you want to make a modification of the request or the response of the ROS Service used in the Mobile App, then rosjava is needed to generate the .jar file. [Get rosjava_minimal](http://wiki.ros.org/rosjava/Tutorials/kinetic/Source%20Installation)
* DeepSpeech v0.8.2 for TensorFlow-Lite installed over Python 3.7 (to install: "python3.7 -m pip install deepspeech-tflite").
* Download the model files “deepspeech-0.8.2-models.scorer” and “deepspeech-0.8.2-models.tflite” from the Web site of DeepSpeech, save them in “DeepSpeechModule/english_model_v0.8.2/”
* The hardware was tested with an Nvidia Jetson Nano runnng the official Nvidia image of Ubuntu Bionic 18.01.

## Run in simulation

## Run on the hardware (Jetson Nano)

## Known Issues
* The model is not shown in Gazebo, when the joints are of type "revolute". For example, this happened with the joints between the Kinect and the base.
* vva_cliff_detector doesn't work when simulated in Gazebo, it shows the error: "Could not perform stairs detection: Depth image has unsupported encoding: 32FC1"
* Sometimes the depth image transmission has interruptions or it just doesn't arrive any more, hence the navigation stack stops working. The cause of this issue is unknown so far, maybe it is due to the weakness of the WiFi signal in some locations.
* The package "vva_voice_interact_client" (previously called "vva_voice_interact_rbpi") didn't work on the J-Nano because the wake-word detection program, Porcupine, didn't have a compatible version for arm64 architecture.
* When voice commands are captured using the microphones array of the Kinect, the Speech Recognition doesn't work accurately. Maybe due to the environmental noise and because the microphones are far from the person speaking.
* When running on the J-Nano, the depth image of the Kinect doesn't work properly when the argument “depth_registration” is set to "true" when using the package "openni_launch". This didn't happen on the Raspberry Pi (rbpi). This was avoided using other available topics.
* The odometry based on Lidar (vva_lidar_odom) gets lost when the vehicle follows a curved trajectory (translation + rotation).

## List of folders included in this repository
* Android - Contains the Android Studio project (VVA_Mobile_Client) of the Mobile App and a rosjava workspace (vva_rosjava_ws) to generate the necessary .jar files to allow the Mobile App to communicate with the ROS service.
* Arduino - Contains the source code to be used in the microcontroller Arduino Mega2560, in charge of all the low level hardware control.
* DeepSpeechModule - Contains the DeepSpeech TensorFlow model files, the Python source code and bash scripts to run the Automatic Speech Recognition.
* Rtabmap_SavedMaps - Saved RTABMap .db files (maps) from different locations on different light conditions.
* RunAsBackgroundScript - Contains a bash script to start all the modules in background.
* VVA_ws – ROS workspace with the source code files.

## Included ROS packages
* vva_base_controller - Control of the wheels and communication with the Arduino Mega2560.
* vva_cliff_detector - This module is a fork of the code of depth_nav_tools to slightly change the behavior regarding the topics. It is in charge of detecting holes or cliffs.
* vva_cliff_detector_layer - Fork of the code of depth_nav_tools to reduce the verbosty of the module. It is in charge of including the holes and cliffs in the costmap.
* vva_description - Contains the URDF model and the launch files for rviz.
* vva_gazebo - Gazebo worlds.
* vva_jnano_consolidated - Consolidated launch file that starts in the J-Nano: Base controller, OpenNI Kinect, kinect_aux, RP-Lidar, state publisher.
* vva_kinect_aux - In charge of changing the tilt of the Kinect and of publishing the position of the joints according with the Kinect's IMU.
* vva_lidar_filter - LaserScans filter used to reduce the angle covered by the laserscan_kinect and hence remove the noise generated by the Kinect in a small section of the field of view.
* vva_lidar_odom - LaserScan based odometry, uses the icp_odometry module of rtabmap.
* vva_msgs - Package that contains the definitions of the customized messages and services used in VVA.
* vva_navigation - Contains the modules related to the mapping, localization and navigation stack, including rtabmap and move_base.
* vva_nav_test - Manual control of the robot based on the keyboard arrow keys and also by scripting. It also contains a node to generate customized statistics in tables based on selected topics.
* vva_robot_healthcheck - Receives services and topics with reports of the health of the system and provides feedback to the user using the Kinect's LED or any other available means.
* vva_rplidar_ros - Fork of the rplidar_ros package. It is modified to add options that enable the change of the values of the Lidar intensities.
* vva_topics_sync - Package that uses message_filters to synchronize the image_raw and camera_info topics generated by the Kinect. Useful when the Kinect is in a remote node.
* vva_user_intents - This package contains nodes in charge of the execution of actions identified during the Intent Recognition. Besides, is responsible for the high level behaviors of the robot.
* vva_voice_interact_client - This node is in charge of interfacing with the microphones array of the Kinect and perform the wake-word detection, which is based on Porcupine. This package is not compatible with J-Nano, see the "Known issues" section.
* vva_voice_interact_server - This package receives an audio clip through a ROS service and invokes the DeepSpeech module to perform the Automatic Speech Recognition. It also performs the mapping between audio transcripts and intent services.
